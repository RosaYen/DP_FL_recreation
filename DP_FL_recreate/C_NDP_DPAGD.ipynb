{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "import random\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 64\n",
    "        self.epochs = 20\n",
    "        self.best_lr_list = []\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 100\n",
    "        self.save_model = False\n",
    "        self.gamma = 0.1\n",
    "        self.alpha_max = 0.1\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('~/data', train=True, download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('~/data', train=False, download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 34, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(34, 64, 5, 1)\n",
    "        self.fc1 = nn.Linear(20*20*64, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1, 20*20*64)\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# model is not exactully the same as the paper since it did not mention the unit of fc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_grad(temp, model):\n",
    "    for net1,net2 in zip(model.named_parameters(),temp.named_parameters()):\n",
    "        net2[1].grad = net1[1].grad.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "args.best_lr_list= [1]*14\n",
    "print(args.best_lr_list)\n",
    "del args.best_lr_list[:]\n",
    "print(args.best_lr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_step_size_model(args, model, device, train_loader):\n",
    "    \n",
    "    r = np.random.randint(920)\n",
    "    sampler = SubsetRandomSampler(list(range(r*args.batch_size, (r+5)*args.batch_size)))\n",
    "    \n",
    "    step_size_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('~/data', train=True, download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    sampler=sampler,\n",
    "    batch_size=args.batch_size,\n",
    "    pin_memory = True\n",
    "    )\n",
    "    \n",
    "    best_loss = math.inf\n",
    "    best_lr = 0\n",
    "    best_model = Net().to(device)\n",
    "    \n",
    "    \n",
    "    if not args.best_lr_list:\n",
    "        args.alpha_max = min((1+args.gamma) * args.alpha_max, 0.1)\n",
    "    elif len(args.best_lr_list) % 10 == 0:\n",
    "        args.alpha_max = max(args.best_lr_list)\n",
    "        del args.best_lr_list[:]\n",
    "    \n",
    "    \n",
    "    for i in np.linspace(0, args.alpha_max, 21):\n",
    "        temp = Net().to(device)\n",
    "        temp_loss = 0\n",
    "        temp.load_state_dict(model.state_dict())\n",
    "        #load_state_dict will not copy the grad, so you need to copy it here.\n",
    "        load_grad(temp, model)\n",
    "        temp_optimizer = optim.SGD(temp.parameters(), lr=i)\n",
    "        temp_optimizer.step()\n",
    "        #optimizer will be new every time, so if you have state in optimizer, it will need load state from the old optimzer.\n",
    "        \n",
    "        for (data, target) in step_size_loader:\n",
    "            data,target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            temp_loss += F.nll_loss(output, target).item()\n",
    "            \n",
    "        if temp_loss < best_loss:\n",
    "            best_model.load_state_dict(temp.state_dict())\n",
    "            best_loss = copy.deepcopy(temp_loss)\n",
    "            best_lr = copy.deepcopy(i)\n",
    "    \n",
    "    args.best_lr_list.append(best_lr)\n",
    "#     print(\"best learning rate:\", best_lr)\n",
    "#     print(\"best loss:\", best_loss)\n",
    "\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, device, model, train_loader, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data,target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Chose the best step size(learning rate)\n",
    "        batch_best_model = best_step_size_model(args, model, device, train_loader)\n",
    "        \n",
    "        model.load_state_dict(batch_best_model.state_dict())\n",
    "        model.zero_grad()\n",
    "        #remember to zero_grad or the grad will accumlate and the model will explode\n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\talpha_max: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(train_loader.dataset) ,\n",
    "                100. * batch_idx * args.batch_size / len(train_loader.dataset), loss.item(), args.alpha_max))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, device, model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader)*(args.batch_size)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / (len(test_loader.dataset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.314098\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.194167\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.140282\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.235297\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.085695\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.220732\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.101401\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.327319\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.168996\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.275723\n",
      "\n",
      "Test set: Average loss: 0.1287, Accuracy: 9601/10000 (96%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.101666\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.145251\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.093275\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.239249\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.064228\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.201880\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.093321\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.283363\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.178420\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.241566\n",
      "\n",
      "Test set: Average loss: 0.1285, Accuracy: 9604/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.101102\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.108101\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.111841\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.221946\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.057331\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.209709\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.098872\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.277816\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.162807\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.281098\n",
      "\n",
      "Test set: Average loss: 0.1285, Accuracy: 9604/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.098995\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.134156\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.107591\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.227335\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.066093\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.208407\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.090827\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.283262\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.183095\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.240574\n",
      "\n",
      "Test set: Average loss: 0.1285, Accuracy: 9604/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.119663\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.117199\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.109198\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.224830\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.061036\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.201797\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.098979\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.305788\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.181930\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.236262\n",
      "\n",
      "Test set: Average loss: 0.1285, Accuracy: 9604/10000 (96%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.114908\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.116389\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.111367\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.214117\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.058408\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.217037\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.102884\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.319417\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.183169\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.255132\n",
      "\n",
      "Test set: Average loss: 0.1285, Accuracy: 9604/10000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.122128\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.130404\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.119509\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.232803\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.065468\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.221163\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.104292\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.287026\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.175414\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.247615\n",
      "\n",
      "Test set: Average loss: 0.1285, Accuracy: 9604/10000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.115622\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.111933\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.099645\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.203402\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.062724\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.223306\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.101689\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.271296\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.182102\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.239346\n",
      "\n",
      "Test set: Average loss: 0.1285, Accuracy: 9604/10000 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.124164\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.119585\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.114387\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.244344\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.058212\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.198962\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.112204\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.272697\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.196733\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.262069\n",
      "\n",
      "Test set: Average loss: 0.1285, Accuracy: 9604/10000 (96%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.116413\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.118290\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.107464\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.212701\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.050498\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.216147\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.094922\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.291218\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.173219\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.241839\n",
      "\n",
      "Test set: Average loss: 0.1285, Accuracy: 9604/10000 (96%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.106801\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.115144\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.114568\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.220575\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.080932\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.204221\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.092138\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.294917\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.162623\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.257864\n",
      "\n",
      "Test set: Average loss: 0.1285, Accuracy: 9604/10000 (96%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.113696\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.126434\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.111351\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.226340\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.066705\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.190468\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.103190\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.318472\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.198925\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.264197\n",
      "\n",
      "Test set: Average loss: 0.1285, Accuracy: 9604/10000 (96%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.118843\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.119941\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.124922\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.211356\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.060367\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.218242\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.101145\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.313667\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.167510\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.277010\n",
      "\n",
      "Test set: Average loss: 0.1285, Accuracy: 9604/10000 (96%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.128336\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.122846\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.112420\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.229401\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.058359\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.210640\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.097233\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "model = Net().to(device)\n",
    "# optimizer =  optim.SGD(model.parameters(), lr=args.lr)\n",
    "args.best_lr_list = []\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    epoch_best_model = train(args, device, model, train_loader , epoch)\n",
    "    model.load_state_dict(epoch_best_model.state_dict())\n",
    "    test(args, device, model, test_loader)\n",
    "\n",
    "if (args.save_model):\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for p1, p2 in zip(model1.parameters(), model2.parameters()):\n",
    "    if p1.data.ne(p2.data).sum() > 0:\n",
    "        print(False)\n",
    "    else: print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
